{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# normalise the data between 0 and 1\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "print(\"Shape of training images:\", train_images.shape)\n",
        "print(\"Shape of training labels:\", train_labels.shape)\n",
        "print(\"Shape of testing images:\", test_images.shape)\n",
        "print(\"Shape of testing labels:\", test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqIz5GPmsnnu",
        "outputId": "755d91bf-4b0e-46f6-cbf3-fb8f7532d858"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training images: (60000, 28, 28)\n",
            "Shape of training labels: (60000,)\n",
            "Shape of testing images: (10000, 28, 28)\n",
            "Shape of testing labels: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 60000\n",
        "test_size = 10000"
      ],
      "metadata": {
        "id": "6YFdTGgAu5at"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape data to a column vector for each training example (28 x 28) to (784 x 1)\n",
        "train_images = train_images.reshape(train_size, 784).T\n",
        "test_images = test_images.reshape(test_size, 784).T\n",
        "\n",
        "train_labels = train_labels.reshape((train_size, 1))\n",
        "test_labels = test_labels.reshape((test_size, 1))\n",
        "\n",
        "# Convert labels to one hot encoding\n",
        "one_hot_train_labels = np.zeros((train_size, 10))\n",
        "one_hot_test_labels = np.zeros((test_size, 10))\n",
        "\n",
        "for i in range(train_size):\n",
        "    one_hot_train_labels[i][train_labels[i]] = 1\n",
        "\n",
        "for i in range(test_size):\n",
        "    one_hot_test_labels[i][test_labels[i]] = 1\n",
        "\n",
        "train_labels = one_hot_train_labels.T\n",
        "test_labels = one_hot_test_labels.T\n",
        "\n",
        "print(\"Updated shape of training images:\", train_images.shape)\n",
        "print(\"Updated shape of training labels:\", train_labels.shape)\n",
        "print(\"Updated shape of testing images:\", test_images.shape)\n",
        "print(\"Updated shape of testing labels:\", test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_on49u-ctPGX",
        "outputId": "64a0964a-3309-44d9-f99d-b7c82c4bfea0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated shape of training images: (784, 60000)\n",
            "Updated shape of training labels: (10, 60000)\n",
            "Updated shape of testing images: (784, 10000)\n",
            "Updated shape of testing labels: (10, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise model parameters\n",
        "\n",
        "w1 = np.random.randn(32, 784) * np.sqrt(1 / 784)\n",
        "w2 = np.random.randn(16, 32) * np.sqrt(1 / 32)\n",
        "w3 = np.random.randn(10, 16) * np.sqrt(1 / 16)\n",
        "\n",
        "b1 = np.zeros((32, 1))\n",
        "b2 = np.zeros((16, 1))\n",
        "b3 = np.zeros((10, 1))"
      ],
      "metadata": {
        "id": "9DLKMUsMuJH_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to perform softmax over a vector x\n",
        "\n",
        "def softmax_stable(x):\n",
        "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())"
      ],
      "metadata": {
        "id": "a51sjjiluTaj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to calculate the derivative of the Relu function on a matrix x\n",
        "\n",
        "def relu_prime(x):\n",
        "    return (x > 0).astype(float)"
      ],
      "metadata": {
        "id": "ihlZpdKEuS2Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to calculate accuracy of y vs y^\n",
        "\n",
        "def calculate_accuracy(predictions, ground_truth):\n",
        "    correct_predictions = np.equal(predictions, ground_truth)\n",
        "    accuracy = np.mean(correct_predictions)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "xZG3avJ8uSA5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x):\n",
        "    a1 = np.matmul(w1, x) + b1\n",
        "    z1 = np.maximum(a1, 0)\n",
        "    \n",
        "    a2 = np.matmul(w2, z1) + b2\n",
        "    z2 = np.maximum(a2,0)\n",
        "\n",
        "    a3 = np.matmul(w3, z2) + b3\n",
        "    z3 = np.apply_along_axis(softmax_stable, axis = 0, arr = a3)\n",
        "\n",
        "    return z3"
      ],
      "metadata": {
        "id": "ePim89XtwHo1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop\n",
        "epochs = 50\n",
        "batch_size = 300\n",
        "learning_rate = 0.01\n",
        "\n",
        "X_train = train_images\n",
        "y_train = train_labels\n",
        "\n",
        "X_test = test_images\n",
        "y_test = test_labels\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for step in range(0, train_size, batch_size):\n",
        "        # Training batch x and y\n",
        "        x = X_train[:, step:step+batch_size]\n",
        "        y = y_train[:, step:step+batch_size]\n",
        "\n",
        "        # Layer 1\n",
        "        a1 = np.matmul(w1, x) + b1\n",
        "        z1 = np.maximum(a1, 0) # Relu\n",
        "\n",
        "        # Layer 2\n",
        "        a2 = np.matmul(w2, z1) + b2\n",
        "        z2 = np.maximum(a2,0) # Relu\n",
        "\n",
        "        # Layer 3 - output\n",
        "        a3 = np.matmul(w3, z2) + b3\n",
        "        z3 = np.apply_along_axis(softmax_stable, axis = 0, arr = a3)\n",
        "\n",
        "        # Calculate gradients for w3\n",
        "        dL_da3 = z3 - y\n",
        "        dL_dw3 = 1/batch_size * np.matmul(dL_da3, z2.T)\n",
        "\n",
        "        # Calculate gradients for w2\n",
        "        dL_dz2 = np.matmul(w3.T, dL_da3)\n",
        "        dL_da2 = np.multiply(dL_dz2, relu_prime(a2))\n",
        "        dL_dw2 = 1/batch_size * np.matmul(dL_da2, z1.T)\n",
        "\n",
        "        # Calculate gradients for w1\n",
        "        dL_dz1 = np.matmul(w2.T, dL_da2)\n",
        "        dL_da1 = np.multiply(dL_dz1, relu_prime(a1))\n",
        "        dL_dw1 = 1/batch_size * np.matmul(dL_da1, x.T)\n",
        "\n",
        "        # Calculate gradients for b3, b2, b1\n",
        "        dL_db3 = 1/batch_size * np.sum(dL_da3, axis = 1).reshape(10, 1)\n",
        "        dL_db2 = 1/batch_size * np.sum(dL_da2, axis = 1).reshape(16, 1)\n",
        "        dL_db1 = 1/batch_size * np.sum(dL_da1, axis = 1).reshape(32, 1)\n",
        "\n",
        "        # Perform parameter updates\n",
        "        w3 = w3 - learning_rate * dL_dw3\n",
        "        w2 = w2 - learning_rate * dL_dw2\n",
        "        w1 = w1 - learning_rate * dL_dw1\n",
        "\n",
        "        b3 = b3 - learning_rate * dL_db3\n",
        "        b2 = b2 - learning_rate * dL_db2\n",
        "        b1 = b1 - learning_rate * dL_db1\n",
        "\n",
        "    print(f'current epoch {epoch}')\n",
        "\n",
        "    # Print average loss on training data\n",
        "    y_pred_train = predict(X_train)\n",
        "    loss = -np.log(y_pred_train[y_train.astype(bool)])\n",
        "    average_loss = np.mean(loss, axis=0)\n",
        "    print(f'Trainig Loss = {average_loss}')\n",
        "\n",
        "    # Print accuracy on test set\n",
        "    y_pred_test = predict(X_test)\n",
        "    accuracy = calculate_accuracy(np.argmax(y_pred_test, axis=0), np.argmax(y_test, axis=0))\n",
        "    print(f'Test Accuracy = {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh_JuZihshL-",
        "outputId": "1e3d54bf-6bfd-4a83-ed55-2e8440cffb7c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current epoch 0\n",
            "Trainig Loss = 0.9100041176856386\n",
            "Test Accuracy = 0.7798\n",
            "current epoch 1\n",
            "Trainig Loss = 0.6788779672681283\n",
            "Test Accuracy = 0.8273\n",
            "current epoch 2\n",
            "Trainig Loss = 0.5681702373949813\n",
            "Test Accuracy = 0.8519\n",
            "current epoch 3\n",
            "Trainig Loss = 0.5024651137450333\n",
            "Test Accuracy = 0.8697\n",
            "current epoch 4\n",
            "Trainig Loss = 0.4585688002885628\n",
            "Test Accuracy = 0.8814\n",
            "current epoch 5\n",
            "Trainig Loss = 0.4274029006703414\n",
            "Test Accuracy = 0.8881\n",
            "current epoch 6\n",
            "Trainig Loss = 0.4043210595150706\n",
            "Test Accuracy = 0.8927\n",
            "current epoch 7\n",
            "Trainig Loss = 0.38664642978400826\n",
            "Test Accuracy = 0.8953\n",
            "current epoch 8\n",
            "Trainig Loss = 0.37261349930991766\n",
            "Test Accuracy = 0.8987\n",
            "current epoch 9\n",
            "Trainig Loss = 0.3611328570762691\n",
            "Test Accuracy = 0.902\n",
            "current epoch 10\n",
            "Trainig Loss = 0.35147462190196327\n",
            "Test Accuracy = 0.9035\n",
            "current epoch 11\n",
            "Trainig Loss = 0.3430874673952659\n",
            "Test Accuracy = 0.9057\n",
            "current epoch 12\n",
            "Trainig Loss = 0.33568036185252903\n",
            "Test Accuracy = 0.9077\n",
            "current epoch 13\n",
            "Trainig Loss = 0.32902762692034965\n",
            "Test Accuracy = 0.9094\n",
            "current epoch 14\n",
            "Trainig Loss = 0.3229481693469774\n",
            "Test Accuracy = 0.9104\n",
            "current epoch 15\n",
            "Trainig Loss = 0.3173247261767713\n",
            "Test Accuracy = 0.9114\n",
            "current epoch 16\n",
            "Trainig Loss = 0.3120642041590802\n",
            "Test Accuracy = 0.9116\n",
            "current epoch 17\n",
            "Trainig Loss = 0.30707068822607175\n",
            "Test Accuracy = 0.913\n",
            "current epoch 18\n",
            "Trainig Loss = 0.3023474936657657\n",
            "Test Accuracy = 0.9145\n",
            "current epoch 19\n",
            "Trainig Loss = 0.297843686527601\n",
            "Test Accuracy = 0.9163\n",
            "current epoch 20\n",
            "Trainig Loss = 0.293519479416954\n",
            "Test Accuracy = 0.9175\n",
            "current epoch 21\n",
            "Trainig Loss = 0.2894004462178298\n",
            "Test Accuracy = 0.9179\n",
            "current epoch 22\n",
            "Trainig Loss = 0.2854389828008521\n",
            "Test Accuracy = 0.9192\n",
            "current epoch 23\n",
            "Trainig Loss = 0.281620224243259\n",
            "Test Accuracy = 0.92\n",
            "current epoch 24\n",
            "Trainig Loss = 0.277940855010278\n",
            "Test Accuracy = 0.9217\n",
            "current epoch 25\n",
            "Trainig Loss = 0.2743620536195543\n",
            "Test Accuracy = 0.9224\n",
            "current epoch 26\n",
            "Trainig Loss = 0.2708757873735967\n",
            "Test Accuracy = 0.9236\n",
            "current epoch 27\n",
            "Trainig Loss = 0.26746989555563994\n",
            "Test Accuracy = 0.9246\n",
            "current epoch 28\n",
            "Trainig Loss = 0.26415270853864514\n",
            "Test Accuracy = 0.9261\n",
            "current epoch 29\n",
            "Trainig Loss = 0.2609088386655859\n",
            "Test Accuracy = 0.9268\n",
            "current epoch 30\n",
            "Trainig Loss = 0.25772069489031296\n",
            "Test Accuracy = 0.9278\n",
            "current epoch 31\n",
            "Trainig Loss = 0.25459299967831606\n",
            "Test Accuracy = 0.9286\n",
            "current epoch 32\n",
            "Trainig Loss = 0.25152428965520374\n",
            "Test Accuracy = 0.9291\n",
            "current epoch 33\n",
            "Trainig Loss = 0.24853122610763673\n",
            "Test Accuracy = 0.9299\n",
            "current epoch 34\n",
            "Trainig Loss = 0.24560032020043226\n",
            "Test Accuracy = 0.9309\n",
            "current epoch 35\n",
            "Trainig Loss = 0.24275199276199785\n",
            "Test Accuracy = 0.9321\n",
            "current epoch 36\n",
            "Trainig Loss = 0.2399933702872436\n",
            "Test Accuracy = 0.9332\n",
            "current epoch 37\n",
            "Trainig Loss = 0.23732462179970984\n",
            "Test Accuracy = 0.9342\n",
            "current epoch 38\n",
            "Trainig Loss = 0.23472826888808762\n",
            "Test Accuracy = 0.9349\n",
            "current epoch 39\n",
            "Trainig Loss = 0.23222371600715577\n",
            "Test Accuracy = 0.9358\n",
            "current epoch 40\n",
            "Trainig Loss = 0.22976493094605954\n",
            "Test Accuracy = 0.9366\n",
            "current epoch 41\n",
            "Trainig Loss = 0.22737977808185306\n",
            "Test Accuracy = 0.937\n",
            "current epoch 42\n",
            "Trainig Loss = 0.22504730113971244\n",
            "Test Accuracy = 0.9373\n",
            "current epoch 43\n",
            "Trainig Loss = 0.22278494062791557\n",
            "Test Accuracy = 0.938\n",
            "current epoch 44\n",
            "Trainig Loss = 0.22059024430081672\n",
            "Test Accuracy = 0.9382\n",
            "current epoch 45\n",
            "Trainig Loss = 0.2184341627850165\n",
            "Test Accuracy = 0.9385\n",
            "current epoch 46\n",
            "Trainig Loss = 0.2163339575358345\n",
            "Test Accuracy = 0.9386\n",
            "current epoch 47\n",
            "Trainig Loss = 0.21428586103669525\n",
            "Test Accuracy = 0.9389\n",
            "current epoch 48\n",
            "Trainig Loss = 0.21227997183266392\n",
            "Test Accuracy = 0.939\n",
            "current epoch 49\n",
            "Trainig Loss = 0.21032527996251998\n",
            "Test Accuracy = 0.9394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See the results of the trained model\n",
        "\n",
        "testing_size = 8\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "random_indices = np.random.choice(test_size, size=testing_size, replace=False)\n",
        "\n",
        "images = X_test.T.reshape((test_size, 28, 28))\n",
        "fig, axes = plt.subplots(nrows=2, ncols=4)\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    ax.imshow(images[random_indices[i]], cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Image {i}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "x = X_test[:, random_indices]\n",
        "y_pred = predict(x)\n",
        "np.argmax(y_pred, axis=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "VRypFFK_yt93",
        "outputId": "f6a323fd-11cf-4a20-d342-f5525780d431"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 9, 2, 3, 6, 3, 7, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare our neural network with the one from keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(-1, 28 * 28) / 255.0\n",
        "x_test = x_test.reshape(-1, 28 * 28) / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(32, activation='relu', input_shape=(28 * 28,)),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=50, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIZ4utxSytq4",
        "outputId": "209eff1a-a481-4a92-9e01-7731fc0e2ade"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "422/422 [==============================] - 7s 4ms/step - loss: 0.5579 - accuracy: 0.8439 - val_loss: 0.2214 - val_accuracy: 0.9382\n",
            "Epoch 2/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.2415 - accuracy: 0.9309 - val_loss: 0.1698 - val_accuracy: 0.9572\n",
            "Epoch 3/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.1973 - accuracy: 0.9431 - val_loss: 0.1501 - val_accuracy: 0.9607\n",
            "Epoch 4/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.1692 - accuracy: 0.9509 - val_loss: 0.1407 - val_accuracy: 0.9607\n",
            "Epoch 5/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.1499 - accuracy: 0.9565 - val_loss: 0.1285 - val_accuracy: 0.9642\n",
            "Epoch 6/50\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.1341 - accuracy: 0.9609 - val_loss: 0.1218 - val_accuracy: 0.9668\n",
            "Epoch 7/50\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.1209 - accuracy: 0.9646 - val_loss: 0.1211 - val_accuracy: 0.9675\n",
            "Epoch 8/50\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.1123 - accuracy: 0.9672 - val_loss: 0.1169 - val_accuracy: 0.9678\n",
            "Epoch 9/50\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.1023 - accuracy: 0.9697 - val_loss: 0.1144 - val_accuracy: 0.9695\n",
            "Epoch 10/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0950 - accuracy: 0.9715 - val_loss: 0.1100 - val_accuracy: 0.9698\n",
            "Epoch 11/50\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.0886 - accuracy: 0.9738 - val_loss: 0.1083 - val_accuracy: 0.9702\n",
            "Epoch 12/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0839 - accuracy: 0.9751 - val_loss: 0.1139 - val_accuracy: 0.9683\n",
            "Epoch 13/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0787 - accuracy: 0.9759 - val_loss: 0.1083 - val_accuracy: 0.9705\n",
            "Epoch 14/50\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0733 - accuracy: 0.9786 - val_loss: 0.1084 - val_accuracy: 0.9695\n",
            "Epoch 15/50\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0707 - accuracy: 0.9786 - val_loss: 0.1137 - val_accuracy: 0.9692\n",
            "Epoch 16/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0675 - accuracy: 0.9796 - val_loss: 0.1082 - val_accuracy: 0.9737\n",
            "Epoch 17/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0629 - accuracy: 0.9810 - val_loss: 0.1115 - val_accuracy: 0.9710\n",
            "Epoch 18/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0608 - accuracy: 0.9816 - val_loss: 0.1143 - val_accuracy: 0.9710\n",
            "Epoch 19/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0575 - accuracy: 0.9829 - val_loss: 0.1213 - val_accuracy: 0.9707\n",
            "Epoch 20/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0545 - accuracy: 0.9836 - val_loss: 0.1180 - val_accuracy: 0.9703\n",
            "Epoch 21/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0525 - accuracy: 0.9840 - val_loss: 0.1207 - val_accuracy: 0.9702\n",
            "Epoch 22/50\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0498 - accuracy: 0.9846 - val_loss: 0.1274 - val_accuracy: 0.9683\n",
            "Epoch 23/50\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0472 - accuracy: 0.9854 - val_loss: 0.1160 - val_accuracy: 0.9728\n",
            "Epoch 24/50\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0456 - accuracy: 0.9857 - val_loss: 0.1280 - val_accuracy: 0.9690\n",
            "Epoch 25/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0431 - accuracy: 0.9870 - val_loss: 0.1192 - val_accuracy: 0.9712\n",
            "Epoch 26/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0422 - accuracy: 0.9871 - val_loss: 0.1230 - val_accuracy: 0.9700\n",
            "Epoch 27/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0403 - accuracy: 0.9876 - val_loss: 0.1242 - val_accuracy: 0.9710\n",
            "Epoch 28/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0379 - accuracy: 0.9885 - val_loss: 0.1262 - val_accuracy: 0.9707\n",
            "Epoch 29/50\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.0360 - accuracy: 0.9896 - val_loss: 0.1325 - val_accuracy: 0.9698\n",
            "Epoch 30/50\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0345 - accuracy: 0.9896 - val_loss: 0.1252 - val_accuracy: 0.9718\n",
            "Epoch 31/50\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0338 - accuracy: 0.9897 - val_loss: 0.1286 - val_accuracy: 0.9718\n",
            "Epoch 32/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0324 - accuracy: 0.9904 - val_loss: 0.1304 - val_accuracy: 0.9723\n",
            "Epoch 33/50\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.0305 - accuracy: 0.9906 - val_loss: 0.1295 - val_accuracy: 0.9723\n",
            "Epoch 34/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0287 - accuracy: 0.9912 - val_loss: 0.1306 - val_accuracy: 0.9718\n",
            "Epoch 35/50\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0275 - accuracy: 0.9918 - val_loss: 0.1361 - val_accuracy: 0.9712\n",
            "Epoch 36/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0268 - accuracy: 0.9917 - val_loss: 0.1337 - val_accuracy: 0.9712\n",
            "Epoch 37/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0268 - accuracy: 0.9919 - val_loss: 0.1409 - val_accuracy: 0.9713\n",
            "Epoch 38/50\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0242 - accuracy: 0.9930 - val_loss: 0.1432 - val_accuracy: 0.9695\n",
            "Epoch 39/50\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0233 - accuracy: 0.9930 - val_loss: 0.1437 - val_accuracy: 0.9695\n",
            "Epoch 40/50\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.0229 - accuracy: 0.9928 - val_loss: 0.1510 - val_accuracy: 0.9713\n",
            "Epoch 41/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9932 - val_loss: 0.1438 - val_accuracy: 0.9715\n",
            "Epoch 42/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9940 - val_loss: 0.1500 - val_accuracy: 0.9687\n",
            "Epoch 43/50\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.1614 - val_accuracy: 0.9702\n",
            "Epoch 44/50\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0177 - accuracy: 0.9950 - val_loss: 0.1507 - val_accuracy: 0.9705\n",
            "Epoch 45/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.1607 - val_accuracy: 0.9688\n",
            "Epoch 46/50\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0183 - accuracy: 0.9946 - val_loss: 0.1657 - val_accuracy: 0.9695\n",
            "Epoch 47/50\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.0169 - accuracy: 0.9951 - val_loss: 0.1617 - val_accuracy: 0.9707\n",
            "Epoch 48/50\n",
            "422/422 [==============================] - 1s 3ms/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.1821 - val_accuracy: 0.9667\n",
            "Epoch 49/50\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.0165 - accuracy: 0.9953 - val_loss: 0.1650 - val_accuracy: 0.9707\n",
            "Epoch 50/50\n",
            "422/422 [==============================] - 1s 4ms/step - loss: 0.0146 - accuracy: 0.9957 - val_loss: 0.1742 - val_accuracy: 0.9687\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1734 - accuracy: 0.9661\n",
            "Test accuracy: 0.9660999774932861\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}